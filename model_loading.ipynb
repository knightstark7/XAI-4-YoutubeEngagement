{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import multiprocessing as mp\n",
    "import cv2\n",
    "from scipy.signal import resample\n",
    "import wave\n",
    "\n",
    "import variables as var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = pd.read_csv(\"prepped_df.csv\")\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_ids(directory):\n",
    "    npz_ids = set()\n",
    "    wav_ids = set()\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".npz\"):\n",
    "            npz_ids.add(file[:-4])  # Remove .npz\n",
    "        elif file.endswith(\".wav\"):\n",
    "            wav_ids.add(file[:-4])  # Remove .wav\n",
    "\n",
    "    common_ids = npz_ids & wav_ids  # Find IDs present in both sets\n",
    "    return sorted(common_ids)\n",
    "\n",
    "existed_ids = find_matching_ids(var.TO_PATH)\n",
    "prep_df = prep_df[prep_df[\"Id\"].isin(existed_ids)]\n",
    "prep_df = prep_df.reset_index(drop=True)\n",
    "len(prep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prep_df['NAWP'], density=True, histtype='step', label='NAWP')\n",
    "plt.hist(prep_df['ECR'], density=True, histtype='step', label='ECR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df['NAWP'].min(), prep_df['NAWP'].max(), prep_df['ECR'].min(), prep_df['ECR'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_audio(video_file, audio_file):\n",
    "    video_data = np.load(video_file)\n",
    "    video_array = video_data[\"video\"]\n",
    "\n",
    "    with wave.open(audio_file, 'r') as wf:\n",
    "        sample_rate = wf.getframerate()\n",
    "        audio_array = np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16)\n",
    "\n",
    "    return video_array, audio_array, sample_rate\n",
    "\n",
    "sample_id = prep_df[\"Id\"].sample(1).values[0]\n",
    "print(f\"Sample ID: {sample_id}\")\n",
    "\n",
    "video_loaded, audio_loaded, sr_loaded = load_video_audio(video_file=f\"{var.TO_PATH}/{sample_id}.npz\", \n",
    "                                                         audio_file=f\"{var.TO_PATH}/{sample_id}.wav\")\n",
    "print(f\"Loaded Video Shape: {video_loaded.shape}\")  # (num_frames, height, width, 3)\n",
    "print(f\"Loaded Audio Shape: {audio_loaded.shape}, Sample Rate: {sr_loaded} Hz\")\n",
    "\n",
    "plt.imshow(video_loaded[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wave\n",
    "import os\n",
    "\n",
    "class VideoAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, prep_path, max_frames, max_audio_samples):\n",
    "        self.dataframe = dataframe\n",
    "        self.prep_path = prep_path\n",
    "        self.ids = dataframe[\"Id\"].values  \n",
    "        self.labels = dataframe[\"ECR\"].values  \n",
    "        self.max_frames = max_frames  \n",
    "        self.max_audio_samples = max_audio_samples  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        video_path = os.path.join(self.prep_path, f\"{sample_id}.npz\")\n",
    "        audio_path = os.path.join(self.prep_path, f\"{sample_id}.wav\")\n",
    "\n",
    "        video_array, audio_array, sample_rate = load_video_audio(video_path, audio_path)\n",
    "\n",
    "        # Convert to tensors\n",
    "        video_tensor = torch.tensor(video_array, dtype=torch.float32).permute(0, 3, 1, 2)  # (num_frames, C, H, W)\n",
    "        audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"id\": sample_id,\n",
    "            \"video\": video_tensor,\n",
    "            \"audio\": audio_tensor,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"label\": label_tensor\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, max_frames, max_audio_samples):\n",
    "    \"\"\"Custom collate function to pad frames and audio.\"\"\"\n",
    "    \n",
    "    video_tensors = []\n",
    "    audio_tensors = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    sample_rates = []\n",
    "\n",
    "    for sample in batch:\n",
    "        video = sample[\"video\"]\n",
    "        audio = sample[\"audio\"]\n",
    "        num_frames = video.shape[0]\n",
    "        num_audio_samples = audio.shape[0]\n",
    "\n",
    "        # Pad video to 300 frames\n",
    "        if num_frames < max_frames:\n",
    "            pad_frames = max_frames - num_frames\n",
    "            padded_video = torch.cat([video, torch.zeros((pad_frames, *video.shape[1:]))], dim=0)\n",
    "        else:\n",
    "            padded_video = video[:max_frames]\n",
    "\n",
    "        # Pad audio to 960,000 samples\n",
    "        if num_audio_samples < max_audio_samples:\n",
    "            pad_audio = max_audio_samples - num_audio_samples\n",
    "            padded_audio = torch.cat([audio, torch.zeros(pad_audio)], dim=0)\n",
    "        else:\n",
    "            padded_audio = audio[:max_audio_samples]\n",
    "\n",
    "        video_tensors.append(padded_video)\n",
    "        audio_tensors.append(padded_audio)\n",
    "        labels.append(sample[\"label\"])\n",
    "        ids.append(sample[\"id\"])\n",
    "        sample_rates.append(sample[\"sample_rate\"])\n",
    "\n",
    "    return {\n",
    "        \"id\": ids,\n",
    "        \"video\": torch.stack(video_tensors),  # (batch_size, num_frames, C, H, W)\n",
    "        \"audio\": torch.stack(audio_tensors),  # (batch_size, audio_length)\n",
    "        \"sample_rate\": sample_rates,\n",
    "        \"label\": torch.stack(labels)  # (batch_size,)\n",
    "    }\n",
    "\n",
    "# Create DataLoader with padding\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    VideoAudioDataset(prep_df, var.TO_PATH, max_frames=var.TARGET_N_FRAME, max_audio_samples=var.TARGET_AUDIO_LENGTH), \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    collate_fn=lambda batch: collate_fn(batch, max_frames=var.TARGET_N_FRAME, max_audio_samples=var.TARGET_AUDIO_LENGTH)\n",
    ")\n",
    "\n",
    "# Sample retrieval\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(sample_batch[\"video\"].shape)  # Expected: (batch_size, num_frames, C, H, W)\n",
    "print(sample_batch[\"audio\"].shape)  # Expected: (batch_size, audio_length)\n",
    "print(sample_batch[\"label\"].shape)  # Expected: (batch_size,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleVideoAudioModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 video_shape=(var.TARGET_N_FRAME, 3, var.TARGET_FRAME_SIZE[0], var.TARGET_FRAME_SIZE[1]), \n",
    "                 audio_length=var.TARGET_AUDIO_LENGTH):\n",
    "        super(SimpleVideoAudioModel, self).__init__()\n",
    "\n",
    "        # Video CNN (3D Conv for spatiotemporal features)\n",
    "        self.video_cnn = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),  # Only reduces H, W\n",
    "\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),\n",
    "            \n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2))\n",
    "        )\n",
    "\n",
    "        # Audio 1D CNN\n",
    "        self.audio_cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        # Compute output sizes\n",
    "        dummy_video = torch.zeros(1, *video_shape)\n",
    "        dummy_audio = torch.zeros(1, audio_length)  # Correct 3D shape\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            video_out = self.video_forward(dummy_video)\n",
    "            audio_out = self.audio_forward(dummy_audio)\n",
    "\n",
    "        feature_dim = video_out.shape[1] + audio_out.shape[1]\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(feature_dim, 1)\n",
    "        \n",
    "        # Final\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def video_forward(self, video):\n",
    "        batch_size, num_frames, C, H, W = video.shape\n",
    "        video = video.permute(0, 2, 1, 3, 4)  # (batch, C, num_frames, H, W)\n",
    "        video_features = self.video_cnn(video)\n",
    "        return video_features.view(batch_size, -1)  # Flatten\n",
    "\n",
    "    def audio_forward(self, audio):\n",
    "        batch_size = audio.shape[0]\n",
    "        audio = audio.unsqueeze(1)\n",
    "        audio_features = self.audio_cnn(audio)  # (batch, channels, time_steps)\n",
    "        return audio_features.view(batch_size, -1)\n",
    "\n",
    "    def forward(self, video, audio):\n",
    "        video_features = self.video_forward(video)\n",
    "        audio_features = self.audio_forward(audio)\n",
    "        combined_features = torch.cat((video_features, audio_features), dim=1)\n",
    "        logits = self.fc(combined_features)\n",
    "        return self.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prep_df[prep_df[\"Set\"] == \"train\"]\n",
    "test_df = prep_df[prep_df[\"Set\"] == \"test\"]\n",
    "\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader with padding\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    VideoAudioDataset(train_df, var.TO_PATH, max_frames=var.TARGET_N_FRAME, max_audio_samples=var.TARGET_AUDIO_LENGTH), \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    collate_fn=lambda batch: collate_fn(batch, max_frames=var.TARGET_N_FRAME, max_audio_samples=var.TARGET_AUDIO_LENGTH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "model = SimpleVideoAudioModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop\n",
    "num_epochs = 5\n",
    "model.to(DEVICE)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for bi, batch in enumerate(train_dataloader):\n",
    "        video = batch[\"video\"].to(DEVICE)  # (batch, num_frames, C, H, W)\n",
    "        audio = batch[\"audio\"].to(DEVICE)  # (batch, audio_length)\n",
    "        labels = batch[\"label\"].to(DEVICE)  # ECR labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(video, audio)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if bi % 10 == 0:\n",
    "            print(labels)\n",
    "            print(outputs)\n",
    "            print(f\"\\tBatch {bi}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    VideoAudioDataset(test_df, var.PREP_PATH, max_frames=var.TARGET_N_FRAME, max_audio_samples=var.TARGET_AUDIO_LENGTH), \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    collate_fn=lambda batch: collate_fn(batch, max_frames=var.TARGET_N_FRAME, max_audio_samples=var.TARGET_AUDIO_LENGTH)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
