{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import multiprocessing as mp\n",
    "import cv2\n",
    "from scipy.signal import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VIDEOS_PATH = \"/mnt/d/Thesis/Data/Video/Train\"\n",
    "TEST_VIDEOS_PATH = \"/mnt/d/Thesis/Data/Video/Test\"\n",
    "\n",
    "TO_PATH = \"/mnt/d/Thesis/Prep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FPS = 1\n",
    "AUDIO_FPS = 2000\n",
    "MAX_SECONDS = 60\n",
    "TARGET_FRAME_SIZE = (480, 840)\n",
    "TARGET_N_FRAME = MAX_SECONDS * VIDEO_FPS\n",
    "TARGET_AUDIO_LENGTH = MAX_SECONDS * AUDIO_FPS\n",
    "\n",
    "\n",
    "SUB_FRACTION = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/SnapUGC/train_out.txt\", sep='\\t')\n",
    "train_df['Set'] = 'train'\n",
    "\n",
    "test_df = pd.read_csv(\"data/SnapUGC/test_out.txt\", sep='\\t')\n",
    "test_df['Set'] = 'test'\n",
    "\n",
    "df = pd.concat([train_df, test_df])\n",
    "\n",
    "display(df)\n",
    "len(train_df), len(test_df), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only select video with duration 10-60s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Video_len'] >= 10) & (df['Video_len'] <= 60)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize order to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmscl = MinMaxScaler()\n",
    "\n",
    "df['ECR'] = mmscl.fit_transform(df['order of ECR'].to_numpy()[:, np.newaxis])[:, 0]\n",
    "df['NAWP'] = mmscl.fit_transform(df['order of NAWP'].to_numpy()[:, np.newaxis])[:, 0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select filtered videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = glob.glob(os.path.join(TRAIN_VIDEOS_PATH, \"*.mp4\"))\n",
    "test_videos = glob.glob(os.path.join(TEST_VIDEOS_PATH, \"*.mp4\"))\n",
    "\n",
    "len(train_videos), len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = pd.DataFrame({\"Id\": [s[s.rfind(\"/\")+1:s.rfind(\".mp4\")] for s in train_videos],\n",
    "                             \"Video\": train_videos,\n",
    "                             \"Set\": \"train\"})\n",
    "test_videos = pd.DataFrame({\"Id\": [s[s.rfind(\"/\")+1:s.rfind(\".mp4\")] for s in test_videos],\n",
    "                             \"Video\": test_videos,\n",
    "                             \"Set\": \"test\"})\n",
    "\n",
    "videos_df = pd.concat([train_videos, test_videos])\n",
    "videos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, videos_df, how='inner', on=['Id', 'Set'])\n",
    "df.set_index('Id', drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['NAWP'], density=True, histtype='step')\n",
    "plt.hist(df['ECR'], density=True, histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['Video_len'], df['NAWP'], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['Video_len'], df['ECR'], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['NAWP'], df['ECR'], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert video to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n=1).iloc[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfrom_image(frames: np.array, target_size=TARGET_FRAME_SIZE, target_n_frames=TARGET_N_FRAME):\n",
    "    resized_frames = np.array([cv2.resize(frame, target_size, interpolation=cv2.INTER_LINEAR) for frame in frames])\n",
    "    # Check if padding is needed\n",
    "    n_frame = resized_frames.shape[0]\n",
    "    if n_frame < target_n_frames:\n",
    "        padding_needed = target_n_frames - n_frame\n",
    "        padding = np.zeros((padding_needed, target_size[1], target_size[0], 3), dtype=np.uint8)\n",
    "        resized_frames = np.concatenate((resized_frames, padding), axis=0)\n",
    "    elif n_frame > target_n_frames:\n",
    "        resized_frames = resized_frames[:target_n_frames]\n",
    "    \n",
    "    return resized_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_audio(audio, fixed_length=TARGET_AUDIO_LENGTH):\n",
    "    length = audio.shape[0]\n",
    "    \n",
    "    if length != fixed_length:\n",
    "        resampled_audio = np.array([resample(audio[:, channel], fixed_length) for channel in range(2)]).T\n",
    "    else:\n",
    "        resampled_audio = audio\n",
    "        \n",
    "    if len(resampled_audio) < fixed_length:\n",
    "        padding_needed = fixed_length - len(resampled_audio)\n",
    "        padding = np.zeros((padding_needed, 2))\n",
    "        resampled_audio = np.vstack([resampled_audio, padding])\n",
    "\n",
    "    return resampled_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def video_to_tensor(path, video_fps=VIDEO_FPS, audio_fps=AUDIO_FPS):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(path)\n",
    "\n",
    "    # Extract frames    \n",
    "    frames = np.array(list(video_clip.iter_frames(fps=video_fps, dtype=\"uint8\"))) # Shape: (num_frames, height, width, 3)\n",
    "    \n",
    "    frames = np.transpose(frames, (3, 0, 1, 2))\n",
    "\n",
    "    # Extract audio as numpy array\n",
    "    audio = video_clip.audio\n",
    "    if audio is not None:\n",
    "        audio_samples = np.array(list(audio.iter_frames(fps=audio_fps)))\n",
    "    else:\n",
    "        audio_samples = None\n",
    "\n",
    "    # Close video to free resources\n",
    "    video_clip.close()\n",
    "    \n",
    "    frames = transfrom_image(frames)\n",
    "    audio_samples = resample_audio(audio_samples)\n",
    "    \n",
    "    return frames, audio_samples\n",
    "\n",
    "\n",
    "frames, audio_samples = video_to_tensor(sample['Video'], VIDEO_FPS, AUDIO_FPS)\n",
    "# Print shapes\n",
    "print(\"Frames shape:\", frames.shape)  # e.g., (num_frames, height, width, 3)\n",
    "if audio_samples is not None:\n",
    "    print(\"Audio shape:\", audio_samples.shape)  # e.g., (num_audio_samples, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[-1])\n",
    "\n",
    "# import sounddevice as sd # remember to install libportaudio2\n",
    "# audio_samples = audio_samples / np.max(np.abs(audio_samples))\n",
    "# print(\"Playing audio...\")\n",
    "# sd.play(audio_samples, samplerate=AUDIO_FPS)\n",
    "# sd.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    name = row.name # id\n",
    "    video_path = row['Video']\n",
    "    ecr = row['ECR']\n",
    "    nawp = row['NAWP']\n",
    "    label = (ecr, nawp)\n",
    "    \n",
    "    # transform\n",
    "    video, audio = video_to_tensor(video_path, VIDEO_FPS, AUDIO_FPS)\n",
    "    # permute to torch format\n",
    "    video = np.transpose(video, (0, 3, 1, 2))\n",
    "    \n",
    "    # target transform\n",
    "    label = np.array(label)\n",
    "    \n",
    "    save_dir = os.path.join(TO_PATH, row['Set'])\n",
    "    \n",
    "    np.savez_compressed(os.path.join(save_dir, f\"{name}_fr.npz\"), video)\n",
    "    np.savez_compressed(os.path.join(save_dir, f\"{name}_audio.npz\"), audio)\n",
    "    np.savez_compressed(os.path.join(save_dir, f\"{name}_label.npz\"), label)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "def main(df: pd.DataFrame, num_processes=None):\n",
    "    with mp.Pool(processes=num_processes if num_processes is not None else mp.cpu_count() - 4) as pool:\n",
    "        # Use imap for incremental updates with tqdm\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                pool.imap(process_row, (row for _, row in df.iterrows())), \n",
    "                total=len(df), \n",
    "                desc=f\"Processing DF\"\n",
    "            )\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(TO_PATH, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TO_PATH, \"test\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df.sample(frac=SUB_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(sub_df, num_processes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from torchvision.transforms import ToTensor\n",
    "\n",
    "# class CustomVideoDataset(Dataset):\n",
    "#     def __init__(self, df: pd.DataFrame, transform=None, target_transform=None, video_fps=VIDEO_FPS, audio_fps=AUDIO_FPS):\n",
    "#         self.df = df\n",
    "#         self.transform = transform\n",
    "#         self.video_fps = video_fps\n",
    "#         self.audio_fps = audio_fps\n",
    "#         self.target_transform = target_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         try:\n",
    "#             row = self.df.iloc[idx]\n",
    "#         except KeyError:\n",
    "#             return None        \n",
    "        \n",
    "#         video_path = row['Video']\n",
    "#         ecr = row['ECR']\n",
    "#         nawp = row['NAWP']\n",
    "#         label = (ecr, nawp)\n",
    "        \n",
    "#         if self.transform:\n",
    "#             video, audio = self.transform(video_path, self.video_fps, self.audio_fps)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(ecr, nawp)\n",
    "        \n",
    "#         # permute frame\n",
    "#         video = torch.permute(torch.tensor(video), (0, 3, 1, 2))\n",
    "        \n",
    "#         return (video, audio), label\n",
    "    \n",
    "# train_set = CustomVideoDataset(df[df['Set'] == 'train'], transform=video_to_tensor)\n",
    "# test_set = CustomVideoDataset(df[df['Set'] == 'test'], transform=video_to_tensor)\n",
    "\n",
    "# len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
