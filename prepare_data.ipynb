{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VIDEOS_PATH = \"/mnt/dat/thes/Train\"\n",
    "TEST_VIDEOS_PATH = \"/mnt/dat/thes/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/SnapUGC/train_out.txt\", sep='\\t')\n",
    "train_df['Set'] = 'train'\n",
    "\n",
    "test_df = pd.read_csv(\"data/SnapUGC/test_out.txt\", sep='\\t')\n",
    "test_df['Set'] = 'test'\n",
    "\n",
    "df = pd.concat([train_df, test_df])\n",
    "\n",
    "display(df)\n",
    "len(train_df), len(test_df), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only select video with duration 10-60s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Video_len'] >= 10) & (df['Video_len'] <= 60)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize order to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmscl = MinMaxScaler()\n",
    "\n",
    "df['ECR'] = mmscl.fit_transform(df['order of ECR'].to_numpy()[:, np.newaxis])[:, 0]\n",
    "df['NAWP'] = mmscl.fit_transform(df['order of NAWP'].to_numpy()[:, np.newaxis])[:, 0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select filtered videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = glob.glob(os.path.join(TRAIN_VIDEOS_PATH, \"*.mp4\"))\n",
    "test_videos = glob.glob(os.path.join(TEST_VIDEOS_PATH, \"*.mp4\"))\n",
    "\n",
    "len(train_videos), len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = pd.DataFrame({\"Id\": [s[s.rfind(\"/\")+1:s.rfind(\".mp4\")] for s in train_videos],\n",
    "                             \"Video\": train_videos,\n",
    "                             \"Set\": \"train\"})\n",
    "test_videos = pd.DataFrame({\"Id\": [s[s.rfind(\"/\")+1:s.rfind(\".mp4\")] for s in test_videos],\n",
    "                             \"Video\": test_videos,\n",
    "                             \"Set\": \"test\"})\n",
    "\n",
    "videos_df = pd.concat([train_videos, test_videos])\n",
    "videos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, videos_df, how='inner', on=['Id', 'Set'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['NAWP'], density=True, histtype='step')\n",
    "plt.hist(df['ECR'], density=True, histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['Video_len'], df['NAWP'], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['Video_len'], df['ECR'], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['NAWP'], df['ECR'], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert video to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FPS = 5\n",
    "AUDIO_FPS = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def video_to_tensor(path, video_fps, audio_fps):\n",
    "    # Load the video file\n",
    "    video_clip = VideoFileClip(path)\n",
    "\n",
    "    # Extract frames    \n",
    "    frames = np.array(list(video_clip.iter_frames(fps=video_fps, dtype=\"uint8\"))) # Shape: (num_frames, height, width, 3)\n",
    "\n",
    "    # Extract audio as numpy array\n",
    "    audio = video_clip.audio\n",
    "    if audio is not None:\n",
    "        audio_samples = np.array(list(audio.iter_frames(fps=audio_fps)))\n",
    "    else:\n",
    "        audio_samples = None\n",
    "\n",
    "    # Close video to free resources\n",
    "    video_clip.close()\n",
    "    \n",
    "    return frames, audio_samples\n",
    "\n",
    "\n",
    "frames, audio_samples = video_to_tensor(df.iloc[2]['Video'], VIDEO_FPS, AUDIO_FPS)\n",
    "# Print shapes\n",
    "print(\"Frames shape:\", frames.shape)  # e.g., (num_frames, height, width, 3)\n",
    "if audio_samples is not None:\n",
    "    print(\"Audio shape:\", audio_samples.shape)  # e.g., (num_audio_samples, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "audio_samples = audio_samples / np.max(np.abs(audio_samples))\n",
    "print(\"Playing audio...\")\n",
    "sd.play(audio_samples, samplerate=AUDIO_FPS)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import librosa.display\n",
    "\n",
    "# # Assume audio_samples is from your `video_to_tensor()` function\n",
    "# if audio_samples is not None:\n",
    "#     # Ensure mono if audio has multiple channels\n",
    "#     if len(audio_samples.shape) == 2:  # Stereo\n",
    "#         audio_samples = np.mean(audio_samples, axis=1)  # Convert to mono\n",
    "\n",
    "#     # Normalize audio to range [-1, 1]\n",
    "#     audio_samples = audio_samples / np.max(np.abs(audio_samples))\n",
    "    \n",
    "#     # Sampling rate from AUDIO_FPS\n",
    "#     sampling_rate = AUDIO_FPS\n",
    "\n",
    "#     # Compute MFCCs\n",
    "#     n_mfcc = 13  # Number of coefficients\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_samples, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "\n",
    "#     # Print MFCC shape\n",
    "#     print(\"MFCCs shape:\", mfccs.shape)  # (n_mfcc, time_frames)\n",
    "\n",
    "#     # Plot MFCCs\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(mfccs, x_axis='time', sr=sampling_rate)\n",
    "#     plt.colorbar(format='%+2.0f dB')\n",
    "#     plt.title('MFCC')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(\"No audio found in the video.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
