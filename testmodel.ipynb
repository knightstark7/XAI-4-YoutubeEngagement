{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10\n",
    "MAX_LENGTH = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "# ðŸ“Œ Define Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize for BLIP\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ðŸ“Œ Load STL-10 Dataset\n",
    "full_dataset = datasets.STL10(root=\"./data\", split=\"train\", transform=transform, download=True)\n",
    "subset_size = int(0.2 * len(full_dataset))  # Small subset for testing\n",
    "indices = np.random.choice(len(full_dataset), subset_size, replace=False)\n",
    "subset_dataset = Subset(full_dataset, indices)\n",
    "\n",
    "# ðŸ“Œ Split into Train (80%) & Test (20%)\n",
    "train_size = int(0.8 * subset_size)\n",
    "test_size = subset_size - train_size\n",
    "train_dataset, test_dataset = random_split(subset_dataset, [train_size, test_size])\n",
    "\n",
    "# ðŸ“Œ Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class_names = [\n",
    "    \"airplane\", \"bird\", \"car\", \"cat\", \"deer\",\n",
    "    \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# ðŸ“Œ Check DataLoader Output\n",
    "for images, labels in train_dataloader:\n",
    "    print(f\"Batch shape: {images.shape}, Labels: {labels}\")\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    break\n",
    "\n",
    "# ðŸ“Œ Select a Sample Batch for Inference\n",
    "sample_images, sample_labels = next(iter(test_dataloader))  # Get a batch of test images\n",
    "\n",
    "os.makedirs(\"sample_batch\", exist_ok=True)\n",
    "\n",
    "# ðŸ“Œ Save Each Image Using Matplotlib\n",
    "for i in range(len(sample_images)):\n",
    "    class_name = class_names[sample_labels[i].item()]  # Get class name\n",
    "    \n",
    "    # Convert Tensor to NumPy Image\n",
    "    img = sample_images[i].permute(1, 2, 0).numpy()  # Change from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot and Save\n",
    "    plt.imshow(img)\n",
    "    plt.title(class_name)  # Set class name as title\n",
    "    plt.savefig(f\"sample_batch/sample_{i}.png\")\n",
    "    plt.close()  # Close figure to avoid memory issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from transformers import ViTFeatureExtractor, ViTModel, BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10\n",
    "MAX_LENGTH = 16\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Class names for STL-10\n",
    "class_names = [\n",
    "    \"airplane\", \"bird\", \"car\", \"cat\", \"deer\",\n",
    "    \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Create directory for saving caption images\n",
    "os.makedirs(\"training_captions\", exist_ok=True)\n",
    "os.makedirs(\"fixed_samples_results\", exist_ok=True)  # New directory for fixed samples\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define the model\n",
    "class ImageTextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, temperature=1.0, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Vision encoder (ViT)\n",
    "        self.vision_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.vision_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "        vision_dim = self.vision_encoder.config.hidden_size\n",
    "        \n",
    "        # Text tokenizer and generation\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Vision to text projection\n",
    "        self.vision_to_text_proj = nn.Linear(vision_dim, vision_dim)\n",
    "        \n",
    "        # Text generation module\n",
    "        self.text_gen_transformer = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=vision_dim, nhead=8, batch_first=True), \n",
    "            num_layers=2\n",
    "        )\n",
    "        self.text_output_layer = nn.Linear(vision_dim, self.vocab_size)\n",
    "        \n",
    "        # Text embeddings for decoder\n",
    "        self.text_embeddings = nn.Embedding(self.vocab_size, vision_dim)\n",
    "        \n",
    "        # Text encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        text_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(text_dim, text_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(text_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Gumbel softmax temperature\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, images, labels=None):\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Vision encoding\n",
    "        vision_outputs = self.vision_encoder(images).last_hidden_state\n",
    "        vision_features = vision_outputs[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Project vision features for text generation\n",
    "        vision_proj = self.vision_to_text_proj(vision_features).unsqueeze(1)\n",
    "        \n",
    "        # Text generation with differentiable sampling\n",
    "        \n",
    "        # Start with batch of [CLS] tokens\n",
    "        start_tokens = torch.full((batch_size, 1), self.tokenizer.cls_token_id, \n",
    "                                  dtype=torch.long, device=images.device)\n",
    "        token_embeds = self.text_embeddings(start_tokens)\n",
    "        \n",
    "        # Storage for text logits and soft token representations\n",
    "        all_text_logits = []\n",
    "        all_soft_tokens = []\n",
    "        \n",
    "        # Autoregressive generation\n",
    "        for step in range(self.max_length - 1):\n",
    "            # Create attention mask\n",
    "            seq_len = token_embeds.size(1)\n",
    "            tgt_mask = (torch.triu(torch.ones(seq_len, seq_len, device=images.device)) == 1).transpose(0, 1)\n",
    "            tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
    "            \n",
    "            # Decode next token\n",
    "            tgt_output = self.text_gen_transformer(\n",
    "                token_embeds, \n",
    "                vision_proj.repeat(1, seq_len, 1),\n",
    "                tgt_mask=tgt_mask\n",
    "            )\n",
    "            \n",
    "            next_token_logits = self.text_output_layer(tgt_output[:, -1:, :])\n",
    "            all_text_logits.append(next_token_logits)\n",
    "            \n",
    "            # Gumbel softmax for differentiable sampling\n",
    "            if self.training:\n",
    "                soft_tokens = F.gumbel_softmax(next_token_logits, tau=self.temperature, hard=False, dim=-1)\n",
    "            else:\n",
    "                indices = torch.argmax(next_token_logits, dim=-1)\n",
    "                soft_tokens = F.one_hot(indices, num_classes=self.vocab_size).float()\n",
    "            \n",
    "            all_soft_tokens.append(soft_tokens)\n",
    "            \n",
    "            # Convert soft tokens to embeddings\n",
    "            next_token_embeds = torch.matmul(soft_tokens, self.text_embeddings.weight)\n",
    "            \n",
    "            # Append to sequence\n",
    "            token_embeds = torch.cat([token_embeds, next_token_embeds], dim=1)\n",
    "        \n",
    "        # Concatenate all tokens\n",
    "        text_logits = torch.cat(all_text_logits, dim=1)\n",
    "        soft_tokens = torch.cat(all_soft_tokens, dim=1)\n",
    "        \n",
    "        # Convert soft tokens to text (always, for sampling during training)\n",
    "        token_indices = torch.argmax(soft_tokens, dim=-1)\n",
    "        full_tokens = torch.cat([start_tokens, token_indices], dim=1)\n",
    "        generated_text = [self.tokenizer.decode(tokens, skip_special_tokens=True) for tokens in full_tokens]\n",
    "        \n",
    "        # Convert to token IDs for BERT\n",
    "        token_ids = torch.cat([start_tokens, torch.argmax(soft_tokens, dim=-1)], dim=1)\n",
    "        attention_mask = torch.ones_like(token_ids)\n",
    "        \n",
    "        # Create soft embeddings\n",
    "        bert_inputs = torch.cat([\n",
    "            self.text_encoder.embeddings.word_embeddings(start_tokens),\n",
    "            torch.matmul(soft_tokens, self.text_encoder.embeddings.word_embeddings.weight)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Forward through BERT\n",
    "        text_outputs = self.text_encoder(\n",
    "            inputs_embeds=bert_inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Classification\n",
    "        text_features = text_outputs.pooler_output\n",
    "        logits = self.classifier(text_features)\n",
    "        \n",
    "        result = {\n",
    "            \"logits\": logits,\n",
    "            \"text_logits\": text_logits,\n",
    "            \"generated_text\": generated_text,\n",
    "        }\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            classification_loss = loss_fn(logits, labels)\n",
    "            result[\"loss\"] = classification_loss\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Helper function to denormalize image and save with caption\n",
    "def save_image_with_caption(image, caption, true_label, pred_label, filename):\n",
    "    # Denormalize the image\n",
    "    img = image.clone().cpu().detach()\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    \n",
    "    # Create figure with caption\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img.permute(1, 2, 0).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Caption: {caption}\\nTrue: {true_label}, Pred: {pred_label}\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Function to evaluate and save results for fixed sample batch\n",
    "def evaluate_fixed_samples(model, fixed_images, fixed_labels, epoch, device=DEVICE):\n",
    "    model.eval()\n",
    "    fixed_images = fixed_images.to(device)\n",
    "    fixed_labels = fixed_labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(fixed_images)\n",
    "        _, predicted = outputs[\"logits\"].max(1)\n",
    "        \n",
    "        # Create a grid of images for this epoch\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10)) if len(fixed_images) >= 8 else plt.subplots(1, len(fixed_images), figsize=(5*len(fixed_images), 5))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i in range(len(fixed_images)):\n",
    "            caption = outputs[\"generated_text\"][i]\n",
    "            true_class = class_names[fixed_labels[i].item()]\n",
    "            pred_class = class_names[predicted[i].item()]\n",
    "            correct = predicted[i].item() == fixed_labels[i].item()\n",
    "            status = \"âœ“\" if correct else \"âœ—\"\n",
    "            \n",
    "            # Individual image\n",
    "            individual_filename = f\"fixed_samples_results/epoch{epoch+1}_sample{i}.png\"\n",
    "            save_image_with_caption(\n",
    "                fixed_images[i], \n",
    "                caption, \n",
    "                true_class, \n",
    "                pred_class, \n",
    "                individual_filename\n",
    "            )\n",
    "            \n",
    "            # For the grid\n",
    "            ax = axes[i]\n",
    "            img = fixed_images[i].cpu()\n",
    "            img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "            \n",
    "            ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "            ax.set_title(f\"Caption: {caption}\\nTrue: {true_class}\\nPred: {pred_class} {status}\", fontsize=9)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Save the grid\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"fixed_samples_results/epoch{epoch+1}_grid.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Return accuracy on fixed set for monitoring\n",
    "        accuracy = (predicted == fixed_labels).sum().item() / len(fixed_labels)\n",
    "        return accuracy, outputs[\"generated_text\"]\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, fixed_samples, optimizer, scheduler, num_epochs=NUM_EPOCHS, device=DEVICE):\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Unpack fixed samples\n",
    "    fixed_images, fixed_labels = fixed_samples\n",
    "    \n",
    "    # Track evolution of captions for all epochs\n",
    "    caption_evolution = {i: [] for i in range(len(fixed_images))}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Create tqdm progress bar with loss tracking\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            \n",
    "            _, predicted = outputs[\"logits\"].max(1)\n",
    "            train_total += labels.size(0)\n",
    "            batch_correct = predicted.eq(labels).sum().item()\n",
    "            train_correct += batch_correct\n",
    "            batch_acc = 100 * batch_correct / labels.size(0)\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'batch_acc': f'{batch_acc:.1f}%',\n",
    "                'avg_acc': f'{100 * train_correct / train_total:.1f}%'\n",
    "            })\n",
    "            \n",
    "            # Save one sample image and caption from each batch\n",
    "            if batch_idx % 10 == 0:  # Save every 10th batch to avoid too many images\n",
    "                sample_idx = 0\n",
    "                caption = outputs[\"generated_text\"][sample_idx]\n",
    "                true_class = class_names[labels[sample_idx].item()]\n",
    "                pred_class = class_names[predicted[sample_idx].item()]\n",
    "                \n",
    "                filename = f\"training_captions/epoch{epoch+1}_batch{batch_idx}.png\"\n",
    "                save_image_with_caption(\n",
    "                    images[sample_idx], \n",
    "                    caption, \n",
    "                    true_class, \n",
    "                    pred_class, \n",
    "                    filename\n",
    "                )\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate on fixed samples after each epoch\n",
    "        print(f\"\\n--- Evaluating fixed samples after epoch {epoch+1} ---\")\n",
    "        fixed_acc, fixed_captions = evaluate_fixed_samples(model, fixed_images, fixed_labels, epoch, device)\n",
    "        \n",
    "        # Store captions for tracking evolution\n",
    "        for i, caption in enumerate(fixed_captions):\n",
    "            caption_evolution[i].append(caption)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        # Create tqdm progress bar for validation\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Validation\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels) in enumerate(val_progress_bar):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images, labels)\n",
    "                loss = outputs[\"loss\"]\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs[\"logits\"].max(1)\n",
    "                val_total += labels.size(0)\n",
    "                batch_correct = predicted.eq(labels).sum().item()\n",
    "                val_correct += batch_correct\n",
    "                \n",
    "                # Update validation progress bar\n",
    "                avg_val_loss = val_loss / (batch_idx + 1)\n",
    "                val_progress_bar.set_postfix({\n",
    "                    'val_loss': f'{avg_val_loss:.4f}',\n",
    "                    'val_acc': f'{100 * val_correct / val_total:.1f}%'\n",
    "                })\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, \"\n",
    "              f\"Fixed Samples Acc: {fixed_acc*100:.2f}%\")\n",
    "        \n",
    "        # Print some generated text examples for fixed samples\n",
    "        print(\"\\nFixed Samples Generated Text:\")\n",
    "        for i in range(min(len(fixed_images), 8)):\n",
    "            true_class = class_names[fixed_labels[i].item()]\n",
    "            print(f\"Sample {i+1} ({true_class}) Caption: {fixed_captions[i]}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"Model saved! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # After all epochs, create caption evolution visualization\n",
    "    plt.figure(figsize=(15, len(fixed_images)*2))\n",
    "    for i in range(min(len(fixed_images), 8)):\n",
    "        plt.subplot(len(fixed_images), 1, i+1)\n",
    "        true_class = class_names[fixed_labels[i].item()]\n",
    "        plt.title(f\"Sample {i+1} ({true_class}) Caption Evolution\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "        for epoch, caption in enumerate(caption_evolution[i]):\n",
    "            plt.text(0, 1-epoch*0.2, f\"Epoch {epoch+1}: {caption}\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fixed_samples_results/caption_evolution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training script\n",
    "def main():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Load STL-10 Dataset\n",
    "    full_dataset = datasets.STL10(root=\"./data\", split=\"train\", transform=transform, download=True)\n",
    "    subset_size = int(0.2 * len(full_dataset))  # Small subset for testing\n",
    "    indices = np.random.choice(len(full_dataset), subset_size, replace=False)\n",
    "    subset_dataset = Subset(full_dataset, indices)\n",
    "\n",
    "    # Split into Train (80%) & Test (20%)\n",
    "    train_size = int(0.8 * subset_size)\n",
    "    test_size = subset_size - train_size\n",
    "    train_dataset, test_dataset = random_split(subset_dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Testing samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Get fixed sample batch for monitoring\n",
    "    sample_images, sample_labels = next(iter(test_dataloader))\n",
    "    \n",
    "    # Save the original fixed samples for reference\n",
    "    os.makedirs(\"fixed_samples_original\", exist_ok=True)\n",
    "    for i in range(len(sample_images)):\n",
    "        img = sample_images[i].clone()\n",
    "        img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(img.permute(1, 2, 0).numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Class: {class_names[sample_labels[i].item()]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"fixed_samples_original/sample_{i}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ImageTextClassifier(num_classes=NUM_CLASSES, temperature=1.0)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trained_model = train_model(\n",
    "        model, train_dataloader, test_dataloader, (sample_images, sample_labels), \n",
    "        optimizer, scheduler, num_epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Final evaluation on fixed samples\n",
    "    print(\"\\nFinal evaluation on fixed samples:\")\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    final_acc, final_captions = evaluate_fixed_samples(model, sample_images, sample_labels, NUM_EPOCHS, DEVICE)\n",
    "    print(f\"Best model accuracy on fixed samples: {final_acc*100:.2f}%\")\n",
    "    \n",
    "    # Create a final comparison grid - original vs. final prediction\n",
    "    fig, axes = plt.subplots(len(sample_images), 2, figsize=(10, len(sample_images)*2.5))\n",
    "    \n",
    "    for i in range(len(sample_images)):\n",
    "        # Original image\n",
    "        img = sample_images[i].clone()\n",
    "        img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        axes[i, 0].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axes[i, 0].set_title(f\"Original: {class_names[sample_labels[i].item()]}\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Final prediction with caption\n",
    "        axes[i, 1].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axes[i, 1].set_title(f\"Caption: {final_captions[i]}\")\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fixed_samples_results/final_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
